streamlit>=1.32
scikit-learn>=1.2
pypdf>=4.0
openai>=1.40
requests>=2.31
transformers>=4.41
accelerate>=0.31
sentencepiece>=0.1.99
safetensors>=0.4.0
torch>=2.0.0
peft>=0.7.0
fastapi>=0.104.0
uvicorn>=0.24.0

# llama.cpp for fast CPU inference with GGUF models
llama-cpp-python>=0.2.0

# Optional: vLLM for high-performance inference (requires NVIDIA GPU)
# Uncomment the line below if you have a GPU:
# vllm>=0.2.7
